{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create local_rag application by langchain and milvus-lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get contents from url\n",
    "\n",
    "We need install langchain-community and beautifulsoup4 packages\n",
    "```\n",
    "pip install -qU langchain-community beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# print(bs4.__version__)\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# The URL of the webpage you want to load\n",
    "page_url = \"YOUR_URL\" # change YOUR_URL to the URL of the webpage you want to load\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "# Send HTTP request to the webpage, and get the HTML code of the webpage\n",
    "response = requests.get(page_url, headers=headers)\n",
    "response.raise_for_status()  # check if the request is successful\n",
    "\n",
    "# use BeautifulSoup to parse the HTML code\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "title_tag = soup.find('h1', class_='rich_media_title')\n",
    "content_tag = soup.find('div', class_='rich_media_content')\n",
    "    # print(title_tag,'and',content_tag)\n",
    "\n",
    "if title_tag and content_tag:\n",
    "    title = title_tag.get_text(strip=True)\n",
    "    content = content_tag.get_text(strip=True)\n",
    "else:\n",
    "    title = 'No title found'\n",
    "    content= 'No content found'\n",
    "\n",
    "print('title:' , title)\n",
    "print(\"content:\" , content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding contents\n",
    "\n",
    "- connect to embedding model\n",
    "The connection parameters for the embedding model are stored in the `.env` file.\n",
    "\n",
    "LangChain provides two methods for using embeddings:\n",
    "1. `embed_query()` — Used to generate embeddings for queries.\n",
    "2. `embed_documents()` — Used to generate embeddings for documents, forming the content of a knowledge base.\n",
    "\n",
    "**Notes**:  \n",
    "1. Pay attention to the maximum dimension value of the current embedding model.  \n",
    "2. Each embedding model has a token limit for processing; documents need to be split based on this limit.  \n",
    "3. The list of documents passed to the embedding model must be in plain text format and cannot be PyString objects.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "# print(os.getenv(\"OPENAI_API_BASE\"))\n",
    "# print(os.getenv(\"OPENAI_MODEL_NAME\"))\n",
    "# print(os.getenv(\"OPENAI_EMB_URL\"))\n",
    "# print(os.getenv(\"OPENAI_EMB_MODEL\"))\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=os.getenv(\"OPENAI_EMB_MODEL\"),\n",
    "    base_url=os.getenv(\"OPENAI_EMB_URL\"),\n",
    "    dimensions=768,\n",
    ")\n",
    "\n",
    "# doc=[]\n",
    "# doc.append(content)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=240,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "docs=text_splitter.create_documents([content])\n",
    "text=[doc.page_content for doc in docs]\n",
    "print(docs)\n",
    "print(text)\n",
    "embeddings = embeddings_model.embed_documents(text)\n",
    "print(embeddings)\n",
    "# print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing the Storage Structure for Vector Data  \n",
    "Using the vector database `Milvus Lite`  \n",
    "```\n",
    "pip install pymilvus\n",
    "```\n",
    "\n",
    "**Note**:  \n",
    "`Milvus Lite` cannot be directly installed and used in a Windows environment.  \n",
    "\n",
    "The data structure in Milvus includes schema, collection, and index.  \n",
    "You need to define the storage structure for vector data first.  \n",
    "\n",
    "In the example below, a collection named `kgbase` will be created.  \n",
    "The vector data dimension is same as dimensions of embedding model.  \n",
    "\n",
    "There are 4 fields in total:  \n",
    "- **id**: `int64` - Store the primary key, `auto_id=True`.  \n",
    "- **docid**: `varchar`, `max_length=512` - Store the document ID or document name.  \n",
    "- **doc_vector**: `float_vector` - the \"dim\" params should be the dimentions of embedding model. Store the contents vector\n",
    "- **doc_content**: `varchar`, `max_length=4096` - Store the document contents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient,DataType,connections,Collection\n",
    "\n",
    "client = MilvusClient(\"./milvus_kgbase.db\")\n",
    "\n",
    "schema = MilvusClient.create_schema(\n",
    "    auto_id=False,\n",
    "    enable_dynamic_field=True,\n",
    ")\n",
    "\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "schema.add_field(field_name=\"docid\", datatype=DataType.VARCHAR, max_length=512),\n",
    "schema.add_field(field_name=\"doc_vector\", datatype=DataType.FLOAT_VECTOR, dim=768),\n",
    "schema.add_field(field_name=\"doc_content\", datatype=DataType.VARCHAR, max_length=4096),\n",
    "\n",
    "index=client.prepare_index_params(\n",
    "    field_name = \"doc_vector\",\n",
    "    index_type = \"AUTOINDEX\",\n",
    "    metric_type = \"COSINE\"\n",
    ")\n",
    "collection_name =\"kgbase\"\n",
    "if client.has_collection(collection_name):\n",
    "    client.drop_collection(collection_name)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    schema=schema,\n",
    "    index_params=index\n",
    ")\n",
    "\n",
    "# res =[]\n",
    "res = client.get_load_state(\n",
    "    collection_name=\"kgbase\"\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Embedded Vectors to the Vector Database  \n",
    "The vector database used is the local `Milvus Lite`.  \n",
    "\n",
    "If any issues occur, remember to restart the Python virtual environment after fixing them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient,DataType,connections,Collection\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    docid = title\n",
    "    doc_vector = embeddings[i]\n",
    "    doc_content = text[i]\n",
    "    data.append({\"docid\":docid,\"doc_vector\":doc_vector,\"doc_content\":doc_content})\n",
    "\n",
    "# for i in range(len(data)):\n",
    "#     print(data[i])\n",
    "\n",
    "client = MilvusClient(\"./milvus_kgbase.db\")\n",
    "res = client.get_load_state(\n",
    "    collection_name=\"kgbase\"\n",
    ")\n",
    "\n",
    "print(res)\n",
    "\n",
    "res = client.insert(\n",
    "    collection_name=\"kgbase\",\n",
    "    data=data\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Retrieval and Answering  \n",
    "\n",
    "This process involves:  \n",
    "1. Generating embeddings for the question.  \n",
    "2. Retrieving the vectorized question from the Milvus database.  \n",
    "3. Combining the retrieved results with the question text into a prompt.  \n",
    "4. Submitting the generated prompt to the large language model for answering. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=os.getenv(\"OPENAI_EMB_MODEL\"),\n",
    "    base_url=os.getenv(\"OPENAI_EMB_URL\"),\n",
    "    dimensions=768,\n",
    ")\n",
    "\n",
    "query=\"YOUR QUERY\"    # Change \"YOUR QUERY\" to your query\n",
    "query_vector=embeddings_model.embed_query(query)\n",
    "print(query_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform retrieval in `Milvus Lite`  \n",
    "Extract the contents from the retrieval results  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient,DataType,connections,Collection\n",
    "\n",
    "client = MilvusClient(\"./milvus_kgbase.db\")\n",
    "collection_name = \"kgbase\"\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"kgbase\",\n",
    "    anns_field=\"doc_vector\",\n",
    "    data=[query_vector],\n",
    "    limit=10,\n",
    "    search_params={\"metric_type\": \"COSINE\"},\n",
    "    output_fields=[\"docid\", \"doc_content\"]  # Specify output fields\n",
    ")\n",
    "\n",
    "# Add the doc_content from the search results to a new list.\n",
    "res_content = []\n",
    "for hits in res:\n",
    "    for hit in hits:\n",
    "        res_content.append(hit['entity']['doc_content'])\n",
    "\n",
    "print(res_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a prompt that includes the search results and the query, then get the answer from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "# dotenv package is import to load the params in the .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Load environment variables\n",
    "os.getenv(\"OPENAI_API_KEY\")\n",
    "os.getenv(\"OPENAI_API_BASE\")\n",
    "os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "\n",
    "llm=OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=os.getenv(\"OPENAI_MODEL_NAME\"),\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    temperature=0.1,\n",
    "#    streaming=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the prompt template\n",
    "template = \"\"\"\n",
    "You are a knowledgeable assistant capable of answering a wide range of questions. \n",
    "Before each question, you will be provided with some reference materials, and you will organize and interpret the information and knowledge points related to the question based on these materials, then answer the question in language that is understandable to the general public.\n",
    "\n",
    "Reference Materials: {contents}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    input_variables=[\"question\",\"contents\"],\n",
    "    template=template,    \n",
    ")\n",
    "\n",
    "contents = \"\"\n",
    "for item in res_content:\n",
    "    # print(item)\n",
    "    contents=contents+\"\\n\"+item\n",
    "\n",
    "# print(contents)\n",
    "\n",
    "generate_prompt=prompt.format(question=query,contents=contents)\n",
    "\n",
    "# print(generate_prompt)\n",
    "\n",
    "response = llm.invoke(generate_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
